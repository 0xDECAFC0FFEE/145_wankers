{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of probabilistic matrix factorisation\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_names = Path(\"dataset\")/\"genome-tags.csv\"                   # tag name lookup\n",
    "movie_review_relevance = Path(\"dataset\")/\"genome-scores.csv\"    # movieid/tagid/relevance\n",
    "movie_genres = Path(\"dataset\")/\"movies.csv\"                     # movieid/movie title/genres\n",
    "reviews = Path(\"dataset\")/\"tags_shuffled_rehashed.csv\"          # userid/movieid/tag\n",
    "train_set = Path(\"dataset\")/\"train_ratings_binary.csv\"          # train set - userid/movieid/ratings\n",
    "val_set = Path(\"dataset\")/\"val_ratings_binary.csv\"              # val set - userid/movieid/ratings\n",
    "test_set = Path(\"dataset\")/\"test_ratings.csv\"                   # test set - userid/movieids\n",
    "\n",
    "NUM_MOVIES = 26744\n",
    "NUM_USERS = 138493\n",
    "NUM_TRAINING_SET = 11946576\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # internal movieids are used as movieids aren't contiguous\n",
    "# userid_uid_lookup = lambda userid: userid-1\n",
    "\n",
    "# movieid_mid_lookup = {}\n",
    "# next_unassigned_mid = 0\n",
    "\n",
    "# def add_movieids_to_lookuptable(filename):\n",
    "#     global next_unassigned_mid\n",
    "\n",
    "#     print(f\"updating lookuptable with mids from {filename}\")\n",
    "#     with open(filename, newline=\"\") as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for rating in tqdm(reader):\n",
    "#             movieid = int(float(rating[\"movieId\"]))\n",
    "#             if movieid not in movieid_mid_lookup:\n",
    "#                 movieid_mid_lookup[movieid] = next_unassigned_mid\n",
    "#                 next_unassigned_mid += 1\n",
    "\n",
    "# add_movieids_to_lookuptable(train_set)\n",
    "# add_movieids_to_lookuptable(val_set)\n",
    "# add_movieids_to_lookuptable(test_set)\n",
    "# add_movieids_to_lookuptable(movie_genres)\n",
    "\n",
    "# with open(\"movieid_mid_lookup\", \"wb+\") as lookup_file:\n",
    "#     pickle.dump(movieid_mid_lookup, lookup_file)\n",
    "\n",
    "userid_uid_lookup = lambda userid: userid-1\n",
    "\n",
    "with open(\"movieid_mid_lookup\", \"rb\") as lookup_file:\n",
    "    movieid_mid_lookup = pickle.load(lookup_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving dataset from dataset/train_ratings_binary.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671f223266ef4c7b9bc2bee82d8752ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "retrieving dataset from dataset/val_ratings_binary.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc56548f0a941baaebcfe4156d7c7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaning up dataset\n",
    "def get_dataset(filename, include_ys=True):\n",
    "    print(f\"retrieving dataset from {filename}\")\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        user_Xs = []\n",
    "        movie_Xs = []\n",
    "        ys = []\n",
    "        for rating in tqdm(reader):\n",
    "            userid = int(float(rating[\"userId\"]))\n",
    "            uid = userid_uid_lookup(userid)\n",
    "            user_Xs.append(uid)\n",
    "            \n",
    "            movieid = int(float(rating[\"movieId\"]))\n",
    "            mid = movieid_mid_lookup[movieid]\n",
    "            movie_Xs.append(mid)\n",
    "            \n",
    "            if include_ys:\n",
    "                score = [1, 0] if (rating[\"rating\"] == \"1\") else [0, 1]\n",
    "                ys.append(score)\n",
    "    if include_ys:\n",
    "        return np.array(user_Xs).reshape(-1, 1), np.array(movie_Xs).reshape(-1, 1), np.array(ys).reshape(-1, 2)\n",
    "    else:\n",
    "        return np.array(user_Xs).reshape(-1, 1), np.array(movie_Xs).reshape(-1, 1)\n",
    "\n",
    "def genre_parser(genre):\n",
    "    if genre == \"(no genres listed)\":\n",
    "        return [\"none/other\"]\n",
    "    return genre.split(\"|\")\n",
    "\n",
    "ALL_GENRES = ['Drama', 'Comedy', 'Thriller', 'Romance', 'Action', 'Crime', 'Horror', 'Documentary', 'Adventure', 'Sci-Fi', 'Mystery', 'Fantasy', 'War', 'Children', 'Musical', 'Animation', 'Western', 'Film-Noir', 'none/other', 'IMAX']\n",
    "with open(movie_genres, newline=\"\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    movie_genres_one_hot = {movieid_mid_lookup[int(float(movie[\"movieId\"]))]: np.array([genre in movie[\"genres\"] for genre in ALL_GENRES]) for movie in reader}        \n",
    "\n",
    "user_Xs, movie_Xs, ys = get_dataset(train_set)\n",
    "user_val_Xs, movie_val_Xs, val_ys = get_dataset(val_set)\n",
    "\n",
    "with open(\"mid_genres_one_hot\", \"wb+\") as genre_file:\n",
    "    pickle.dump(movie_genres_one_hot, genre_file)\n",
    "with open(\"training_set\", \"wb+\") as training_set_file:\n",
    "    pickle.dump((user_Xs, movie_Xs, ys), training_set_file)\n",
    "with open(\"val_set\", \"wb+\") as val_set_file:\n",
    "    pickle.dump((user_val_Xs, movie_val_Xs, val_ys), val_set_file)\n",
    "\n",
    "with open(\"mid_genres_one_hot\", \"rb\") as genre_file:\n",
    "    movie_genres_one_hot = pickle.load(genre_file)\n",
    "with open(\"training_set\", \"rb\") as training_set_file:\n",
    "    user_Xs, movie_Xs, ys = pickle.load(training_set_file)\n",
    "with open(\"val_set\", \"rb\") as val_set_file:\n",
    "    user_val_Xs, movie_val_Xs, val_ys = pickle.load(val_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(*args, batch_size=1000, shuffle=True):\n",
    "    if batch_size == -1:\n",
    "        return [args]\n",
    "    \n",
    "    num_elems = len(args[0])\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.arange(num_elems, dtype=np.int64)\n",
    "        np.random.shuffle(shuffle_indices)\n",
    "        for i in range(0, num_elems, batch_size):\n",
    "            array_indices = shuffle_indices[i: i+batch_size]\n",
    "            try:\n",
    "                yield [arg[array_indices] for arg in args]\n",
    "            except:\n",
    "                raise Exception(\"args to batchify must be numpy arrays if shuffle True\")\n",
    "    else:\n",
    "        for i in range(0, num_elems, batch_size):\n",
    "            yield [arg[i: i+batch_size] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all movies in test data accounted for in genre information dataset\n",
    "\n",
    "# no_genre_count = 0\n",
    "# total = 0\n",
    "\n",
    "# with open(test_set, newline=\"\") as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "#     for rating in tqdm(reader):\n",
    "#         if movieid_mid_lookup[int(float(rating[\"movieId\"]))] not in movie_genres_one_hot:\n",
    "#             no_genre_count += 1\n",
    "#         total += 1\n",
    "\n",
    "# print(f\"{no_genre_count}/{total} entries in the test data doesn't have genre info ({no_genre_count/total}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # no memory - implicitly calculating user movie matrix from now on\n",
    "\n",
    "# movie_embeddings = tf.Variable(tf.random_normal([5, NUM_MOVIES], stddev=0.03, dtype=tf.float32))\n",
    "# user_embeddings = tf.Variable(tf.random_normal([NUM_USERS, 5], stddev=0.03, dtype=tf.float32))\n",
    "# movie_bias = tf.Variable(tf.random_normal([1, NUM_MOVIES], stddev=0.03, dtype=tf.float32))\n",
    "# user_bias = tf.Variable(tf.random_normal([NUM_USERS, 1], stddev=0.03, dtype=tf.float32))\n",
    "\n",
    "# user_movie_score = tf.tensordot(user_embeddings, movie_embeddings, axes = 1)+.14*tf.tile(movie_bias, [NUM_USERS, 1]) +.87*tf.tile(user_bias, [1, NUM_MOVIES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_embedding_rows shape (?, 40)\n",
      "(?, 40) (?, 40) (?, 1)\n",
      "input layer shape (?, 142)\n",
      "pred_y shape (?, 2)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 40\n",
    "assert embedding_dim > 20\n",
    "\n",
    "movie_genre_embeddings = tf.placeholder(dtype=tf.float32, shape=[None, 20])\n",
    "movie_embeddings = tf.Variable(tf.contrib.layers.xavier_initializer()([NUM_MOVIES, embedding_dim]))\n",
    "user_embeddings = tf.Variable(tf.contrib.layers.xavier_initializer()([NUM_USERS, embedding_dim]))\n",
    "movie_bias = tf.Variable(tf.random_normal([NUM_MOVIES], stddev=0.03, dtype=tf.float32))\n",
    "user_bias = tf.Variable(tf.random_normal([NUM_USERS], stddev=0.03, dtype=tf.float32))\n",
    "\n",
    "user_slice_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1]) # columns vectors to do tensor slicing\n",
    "movie_slice_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1]) # columns vectors to do tensor slicing\n",
    "user_bias_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1]) # columns vectors to do tensor slicing\n",
    "movie_bias_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1]) # columns vectors to do tensor slicing\n",
    "\n",
    "user_embedding_columns = tf.reshape(tf.gather_nd(user_embeddings, user_slice_idxs), [-1, embedding_dim])\n",
    "movie_embedding_rows = tf.reshape(tf.gather_nd(movie_embeddings, movie_slice_idxs), [-1, embedding_dim])\n",
    "print(\"movie_embedding_rows shape\", movie_embedding_rows.shape)\n",
    "\n",
    "user_slice_bias = tf.reshape(tf.gather_nd(user_bias, user_slice_idxs), [-1, 1])\n",
    "movie_slice_bias = tf.reshape(tf.gather_nd(movie_bias, movie_slice_idxs), [-1, 1])\n",
    "# print(\"user_slice_bias shape\", user_slice_bias.shape)\n",
    "\n",
    "# print((user_embedding_columns * tf.concat((movie_embedding_rows, movie_genre_embeddings), axis=1)).shape)\n",
    "# print(movie_slice_bias.shape)\n",
    "# print(user_slice_bias.shape)\n",
    "\n",
    "input_layer = tf.concat((\n",
    "    movie_embedding_rows * user_embedding_columns,\n",
    "    movie_embedding_rows,\n",
    "    movie_genre_embeddings,\n",
    "    user_embedding_columns,\n",
    "    user_slice_bias,\n",
    "    movie_slice_bias), axis=1)\n",
    "print(movie_embedding_rows.shape, user_embedding_columns.shape, user_slice_bias.shape)\n",
    "print(\"input layer shape\", input_layer.shape)\n",
    "\n",
    "W1 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[embedding_dim*3+2+20, 40], dtype=tf.float32))\n",
    "b1 = tf.Variable(initial_value=np.zeros(shape=[40], dtype=np.float32))\n",
    "l1 = tf.nn.relu(tf.matmul(input_layer, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[40, 20], dtype=tf.float32))\n",
    "b2 = tf.Variable(initial_value=np.zeros(shape=[20], dtype=np.float32))\n",
    "l2 = tf.nn.relu(tf.matmul(l1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[20, 2], dtype=tf.float32))\n",
    "b3 = tf.Variable(initial_value=np.zeros(shape=[2], dtype=np.float32))\n",
    "pred_y =tf.nn.sigmoid(tf.matmul(l2, W3) + b3)\n",
    "\n",
    "print(\"pred_y shape\", pred_y.shape)\n",
    "\n",
    "# embedding_pred_vectors = tf.reshape(tf.reduce_sum(user_embedding_columns * tf.concat((movie_embedding_rows, movie_genre_embeddings), axis=1), axis=1), (-1, 1))\n",
    "# pred_y = embedding_pred_vectors + .14*movie_slice_bias + .87*user_slice_bias\n",
    "# print(embedding_pred_vectors.shape)\n",
    "# print(pred_y.shape)\n",
    "\n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[None, 2])\n",
    "\n",
    "all_weights = [W1, W2, W3, b1, b2, b3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing train val genres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60772756fdb469fac298b07bb2aa6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11946576), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb07c0e505844da879fd5c032c9417a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3999236), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd99658364e4c51ba907e7078a52d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "train loss 0.25048026 pred_ys [[0.4486436  0.45592758]\n",
      " [0.4875048  0.4856502 ]\n",
      " [0.49498045 0.50553006]\n",
      " [0.55135083 0.49642667]\n",
      " [0.4872741  0.5200228 ]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.009432259 mse_loss_term 0.25048026\n",
      "val loss 0.26032788\n",
      "val acc 0.5276325278128122\n",
      "val auc 0.5395679954095511\n",
      "training\n",
      "train loss 0.25552925 pred_ys [[0.70468754 0.584589  ]\n",
      " [0.7308224  0.4343065 ]\n",
      " [0.71024585 0.46933824]\n",
      " [0.61606896 0.61227685]\n",
      " [0.6437297  0.58311886]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.011769414 mse_loss_term 0.25552925\n",
      "val loss 0.23878416\n",
      "val acc 0.6225221517309806\n",
      "val auc 0.6132963530826474\n",
      "training\n",
      "train loss 0.22869205 pred_ys [[0.16821742 0.64915633]\n",
      " [0.5175473  0.20806903]\n",
      " [0.30197698 0.49760517]\n",
      " [0.08804232 0.6163841 ]\n",
      " [0.20575282 0.61620516]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.016070614 mse_loss_term 0.22869205\n",
      "val loss 0.21611896\n",
      "val acc 0.6797145754839174\n",
      "val auc 0.6808954560128517\n",
      "training\n",
      "train loss 0.2106879 pred_ys [[0.38190883 0.62430036]\n",
      " [0.9336237  0.01798043]\n",
      " [0.71641076 0.23312768]\n",
      " [0.07199478 0.8461579 ]\n",
      " [0.25082093 0.7133335 ]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.022900457 mse_loss_term 0.2106879\n",
      "val loss 0.20350203\n",
      "val acc 0.6951292696905109\n",
      "val auc 0.6955185056582963\n",
      "training\n",
      "train loss 0.19055653 pred_ys [[0.46092322 0.63569486]\n",
      " [0.8549515  0.05505145]\n",
      " [0.5645617  0.43309644]\n",
      " [0.06949812 0.97898126]\n",
      " [0.18078297 0.91290396]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.03070758 mse_loss_term 0.19055653\n",
      "val loss 0.20220438\n",
      "val acc 0.6892771519360198\n",
      "val auc 0.6858651296091653\n",
      "training\n",
      "train loss 0.19102599 pred_ys [[0.4796426  0.62355465]\n",
      " [0.60236907 0.30912155]\n",
      " [0.48263904 0.5619197 ]\n",
      " [0.11396101 0.9859563 ]\n",
      " [0.14079675 0.974452  ]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.037955713 mse_loss_term 0.19102599\n",
      "val loss 0.19738176\n",
      "val acc 0.6973959526269518\n",
      "val auc 0.6960522496848739\n",
      "training\n",
      "train loss 0.18544108 pred_ys [[0.4902607  0.59111524]\n",
      " [0.7457493  0.19127873]\n",
      " [0.49093425 0.5404818 ]\n",
      " [0.16158152 0.9707279 ]\n",
      " [0.10835025 0.98395115]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.04516097 mse_loss_term 0.18544108\n",
      "val loss 0.19572803\n",
      "val acc 0.7008758672906525\n",
      "val auc 0.7007777270502507\n",
      "training\n",
      "train loss 0.18258566 pred_ys [[0.4916921  0.5722725 ]\n",
      " [0.8597493  0.08925733]\n",
      " [0.4790093  0.53642863]\n",
      " [0.10276678 0.9646367 ]\n",
      " [0.04722381 0.98901296]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.053049218 mse_loss_term 0.18258566\n",
      "val loss 0.19479197\n",
      "val acc 0.7046385859699202\n",
      "val auc 0.7046471727519867\n",
      "training\n",
      "train loss 0.18073651 pred_ys [[0.4575986  0.5959674 ]\n",
      " [0.9256492  0.04778087]\n",
      " [0.43372786 0.55765724]\n",
      " [0.03896478 0.9653981 ]\n",
      " [0.01337627 0.990751  ]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.061274435 mse_loss_term 0.18073651\n",
      "val loss 0.1930864\n",
      "val acc 0.7069067691929158\n",
      "val auc 0.7066062711856125\n",
      "training\n",
      "train loss 0.17874719 pred_ys [[0.40109482 0.621456  ]\n",
      " [0.94059765 0.04784235]\n",
      " [0.4209708  0.54393935]\n",
      " [0.02753231 0.9286665 ]\n",
      " [0.01003963 0.9762669 ]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.069455616 mse_loss_term 0.17874719\n",
      "val loss 0.19151263\n",
      "val acc 0.7084618162068955\n",
      "val auc 0.7080393345357895\n",
      "training\n",
      "train loss 0.17697084 pred_ys [[0.3450957  0.6400258 ]\n",
      " [0.93099964 0.0728223 ]\n",
      " [0.45179114 0.5246062 ]\n",
      " [0.02983496 0.88565326]\n",
      " [0.02384678 0.91503596]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.07749231 mse_loss_term 0.17697084\n",
      "val loss 0.19054155\n",
      "val acc 0.7094182488855372\n",
      "val auc 0.7090421241345296\n",
      "training\n",
      "train loss 0.17564037 pred_ys [[0.28291148 0.6884431 ]\n",
      " [0.90952694 0.11521578]\n",
      " [0.48655853 0.51672655]\n",
      " [0.03250688 0.91696894]\n",
      " [0.0407432  0.91245824]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.08569691 mse_loss_term 0.17564037\n",
      "val loss 0.18993491\n",
      "val acc 0.7102774129858803\n",
      "val auc 0.7102805786535653\n",
      "training\n",
      "train loss 0.17465009 pred_ys [[0.25044486 0.73451984]\n",
      " [0.89754784 0.13981178]\n",
      " [0.500035   0.5108398 ]\n",
      " [0.03301075 0.9502655 ]\n",
      " [0.04848629 0.9325253 ]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.0939907 mse_loss_term 0.17465009\n",
      "val loss 0.18916757\n",
      "val acc 0.7101686422106622\n",
      "val auc 0.7111755699004823\n",
      "training\n",
      "train loss 0.17338963 pred_ys [[0.25737858 0.7488857 ]\n",
      " [0.9075877  0.1239734 ]\n",
      " [0.50585496 0.504705  ]\n",
      " [0.02948451 0.97013927]\n",
      " [0.04445961 0.9507227 ]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.10259159 mse_loss_term 0.17338963\n",
      "val loss 0.1888861\n",
      "val acc 0.7111425782324424\n",
      "val auc 0.7122691453727734\n",
      "training\n",
      "train loss 0.17244107 pred_ys [[0.27513063 0.7483951 ]\n",
      " [0.91452664 0.10295215]\n",
      " [0.5058815  0.5020673 ]\n",
      " [0.02214417 0.98099846]\n",
      " [0.03327134 0.9710329 ]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.11166637 mse_loss_term 0.17244107\n",
      "val loss 0.1887042\n",
      "val acc 0.7121457698420398\n",
      "val auc 0.7128802181269995\n",
      "training\n",
      "train loss 0.17147945 pred_ys [[0.2853309  0.7482538 ]\n",
      " [0.89870787 0.10499796]\n",
      " [0.46837083 0.53428   ]\n",
      " [0.01678884 0.9876516 ]\n",
      " [0.02337328 0.9828212 ]] true_ys [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "l2_loss_term 0.12106016 mse_loss_term 0.17147945\n",
      "val loss 0.18813205\n"
     ]
    }
   ],
   "source": [
    "# def compute_slices(user_Xs, movie_Xs, embedding_dim):\n",
    "#     user_slice_idxs = [[user_X] for user_X in tqdm(user_Xs)]\n",
    "#     movie_slice_idxs = [[movie_X] for movie_X in tqdm(movie_Xs)]\n",
    "    \n",
    "    \n",
    "#     return [np.array(user_slice_idxs).reshape([-1, 1]), np.array(movie_slice_idxs).reshape([-1, 1])]\n",
    "\n",
    "learning_rate = .1\n",
    "epochs = 35\n",
    "\n",
    "l2_loss_term = .0001 * sum([tf.reduce_sum(tf.reshape(weight*weight, [-1])) for weight in all_weights])\n",
    "mse_loss_term = tf.reduce_mean(tf.squared_difference(pred_y, y_true))\n",
    "loss = mse_loss_term\n",
    "# + l2_loss_term\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# print(\"computing train val slice indices\")\n",
    "# slice_indices = compute_slices(user_Xs, movie_Xs, embedding_dim)\n",
    "# user_slice, movie_slice = slice_indices\n",
    "\n",
    "# val_slice_indices = compute_slices(user_val_Xs, movie_val_Xs, embedding_dim)\n",
    "# user_val_slice, movie_val_slice = val_slice_indices\n",
    "\n",
    "user_val_slice, movie_val_slice = user_val_Xs, movie_val_Xs\n",
    "print(\"computing train val genres\")\n",
    "train_genres = np.array([movie_genres_one_hot[x[0]] for x in tqdm(movie_Xs)])\n",
    "val_genres = np.array([movie_genres_one_hot[x[0]] for x in tqdm(movie_val_Xs)])\n",
    "\n",
    "flat_val_ys = [1 if y[0] > y[1] else 0 for y in val_ys]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), leave=False):\n",
    "        feed_dict = {user_slice_idxs: user_slice, \n",
    "                     movie_slice_idxs: movie_slice, \n",
    "                     movie_genre_embeddings: train_genres,\n",
    "                     y_true: ys}\n",
    "        print(\"training\")\n",
    "        outs = (train_step, loss, pred_y[:5], l2_loss_term, mse_loss_term)\n",
    "        _, lossval, pred_y_val, l2_loss_term_val, mse_loss_term_val = sess.run(outs, feed_dict=feed_dict)\n",
    "        print(\"train loss\", lossval, \"pred_ys\", pred_y_val, \"true_ys\", ys[:5])\n",
    "        print(\"l2_loss_term\", l2_loss_term_val, \"mse_loss_term\", mse_loss_term_val)\n",
    "        \n",
    "        feed_dict = {user_slice_idxs: user_val_slice, \n",
    "                         movie_slice_idxs: movie_val_slice,\n",
    "                         movie_genre_embeddings: val_genres,\n",
    "                         y_true: val_ys}\n",
    "        val_y_pred, val_loss_val = sess.run((pred_y, loss), feed_dict=feed_dict)\n",
    "        flat_val_y_pred = [1 if pred[0] > pred[1] else 0 for pred in val_y_pred]\n",
    "        print(\"val loss\", val_loss_val)\n",
    "        print(\"val acc\", metrics.accuracy_score(flat_val_ys, flat_val_y_pred))\n",
    "        fpr, tpr, _ = metrics.roc_curve(flat_val_ys, flat_val_y_pred)\n",
    "        print(\"val auc\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc [0.51717003]\n",
      "val acc? 0.5171700294756298\n"
     ]
    }
   ],
   "source": [
    "total_val_correct = [((1 if pred > .5 else -1) == true) for pred, true in zip(val_y_pred, val_ys)]\n",
    "print(\"val acc\", sum(total_val_correct)/len(val_ys))\n",
    "print(\"val acc?\", metrics.accuracy_score([(1 if (i > .5) else -1) for i in val_y_pred], val_ys.T[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1] [ 1 -1 -1  1  1]\n"
     ]
    }
   ],
   "source": [
    "print([(1 if i else -1) for i in val_y_pred][:5], val_ys.T[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3999236, 1) (3999236, 1) (3999236,)\n",
      "(11946576, 1) (11946576, 1) (11946576,)\n"
     ]
    }
   ],
   "source": [
    "val_slice_indices = compute_slices(user_val_Xs, movie_val_Xs, embedding_dim)\n",
    "user_val_slice, movie_val_slice = val_slice_indices\n",
    "\n",
    "print(user_val_slice.shape, movie_val_slice.shape, np.array(val_ys).shape)\n",
    "\n",
    "slice_indices = compute_slices(user_Xs, movie_Xs, embedding_dim)\n",
    "user_slice, movie_slice = slice_indices\n",
    "\n",
    "print(user_slice.shape, movie_slice.shape, np.array(ys).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11946576 11946576\n",
      "3999236 3999236\n"
     ]
    }
   ],
   "source": [
    "print(len(user_Xs), len(ys))\n",
    "print(len(user_val_Xs), len(val_ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
