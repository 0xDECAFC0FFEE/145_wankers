{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of probabilistic matrix factorisation\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_names = Path(\"dataset\")/\"genome-tags.csv\"                   # tag name lookup\n",
    "movie_review_relevance = Path(\"dataset\")/\"genome-scores.csv\"    # movieid/tagid/relevance\n",
    "movie_genres = Path(\"dataset\")/\"movies.csv\"                     # movieid/movie title/genres\n",
    "reviews = Path(\"dataset\")/\"tags_shuffled_rehashed.csv\"          # userid/movieid/tag\n",
    "train_set = Path(\"dataset\")/\"train_ratings_binary.csv\"          # train set - userid/movieid/ratings\n",
    "val_set = Path(\"dataset\")/\"val_ratings_binary.csv\"              # val set - userid/movieid/ratings\n",
    "test_set = Path(\"dataset\")/\"test_ratings.csv\"                   # test set - userid/movieids\n",
    "\n",
    "NUM_MOVIES = 26744\n",
    "NUM_USERS = 138493\n",
    "NUM_TRAINING_SET = 11946576\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # internal movieids are used as movieids aren't contiguous\n",
    "# userid_uid_lookup = lambda userid: userid-1\n",
    "\n",
    "# movieid_mid_lookup = {}\n",
    "# next_unassigned_mid = 0\n",
    "\n",
    "# def add_movieids_to_lookuptable(filename):\n",
    "#     global next_unassigned_mid\n",
    "\n",
    "#     print(f\"updating lookuptable with mids from {filename}\")\n",
    "#     with open(filename, newline=\"\") as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for rating in tqdm(reader):\n",
    "#             movieid = int(float(rating[\"movieId\"]))\n",
    "#             if movieid not in movieid_mid_lookup:\n",
    "#                 movieid_mid_lookup[movieid] = next_unassigned_mid\n",
    "#                 next_unassigned_mid += 1\n",
    "\n",
    "# add_movieids_to_lookuptable(train_set)\n",
    "# add_movieids_to_lookuptable(val_set)\n",
    "# add_movieids_to_lookuptable(test_set)\n",
    "# add_movieids_to_lookuptable(movie_genres)\n",
    "\n",
    "# with open(\"movieid_mid_lookup\", \"wb+\") as lookup_file:\n",
    "#     pickle.dump(movieid_mid_lookup, lookup_file)\n",
    "\n",
    "userid_uid_lookup = lambda userid: userid-1\n",
    "\n",
    "with open(\"movieid_mid_lookup\", \"rb\") as lookup_file:\n",
    "    movieid_mid_lookup = pickle.load(lookup_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving dataset from dataset/train_ratings_binary.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f9b9b27cb74feeadadd0ce9cf40d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "retrieving dataset from dataset/val_ratings_binary.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da41a334e764364b4ae867c658e5dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaning up dataset\n",
    "def get_dataset(filename, include_ys=True):\n",
    "    print(f\"retrieving dataset from {filename}\")\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        user_Xs = []\n",
    "        movie_Xs = []\n",
    "        ys = []\n",
    "        for rating in tqdm(reader):\n",
    "            userid = int(float(rating[\"userId\"]))\n",
    "            uid = userid_uid_lookup(userid)\n",
    "            user_Xs.append(uid)\n",
    "            \n",
    "            movieid = int(float(rating[\"movieId\"]))\n",
    "            mid = movieid_mid_lookup[movieid]\n",
    "            movie_Xs.append(mid)\n",
    "            \n",
    "            if include_ys:\n",
    "                score = 1 if (rating[\"rating\"] == \"1\") else -1\n",
    "                ys.append(score)\n",
    "    if include_ys:\n",
    "        return np.array(user_Xs).reshape(-1, 1), np.array(movie_Xs).reshape(-1, 1), np.array(ys).reshape(-1, 1)\n",
    "    else:\n",
    "        return np.array(user_Xs).reshape(-1, 1), np.array(movie_Xs).reshape(-1, 1)\n",
    "\n",
    "def genre_parser(genre):\n",
    "    if genre == \"(no genres listed)\":\n",
    "        return [\"none/other\"]\n",
    "    return genre.split(\"|\")\n",
    "\n",
    "ALL_GENRES = ['Drama', 'Comedy', 'Thriller', 'Romance', 'Action', 'Crime', 'Horror', 'Documentary', 'Adventure', 'Sci-Fi', 'Mystery', 'Fantasy', 'War', 'Children', 'Musical', 'Animation', 'Western', 'Film-Noir', 'none/other', 'IMAX']\n",
    "with open(movie_genres, newline=\"\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    movie_genres_one_hot = {movieid_mid_lookup[int(float(movie[\"movieId\"]))]: np.array([genre in movie[\"genres\"] for genre in ALL_GENRES]) for movie in reader}        \n",
    "\n",
    "user_Xs, movie_Xs, ys = get_dataset(train_set)\n",
    "user_val_Xs, movie_val_Xs, val_ys = get_dataset(val_set)\n",
    "\n",
    "with open(\"mid_genres_one_hot\", \"wb+\") as genre_file:\n",
    "    pickle.dump(movie_genres_one_hot, genre_file)\n",
    "with open(\"training_set\", \"wb+\") as training_set_file:\n",
    "    pickle.dump((user_Xs, movie_Xs, ys), training_set_file)\n",
    "with open(\"val_set\", \"wb+\") as val_set_file:\n",
    "    pickle.dump((user_val_Xs, movie_val_Xs, val_ys), val_set_file)\n",
    "\n",
    "with open(\"mid_genres_one_hot\", \"rb\") as genre_file:\n",
    "    movie_genres_one_hot = pickle.load(genre_file)\n",
    "with open(\"training_set\", \"rb\") as training_set_file:\n",
    "    user_Xs, movie_Xs, ys = pickle.load(training_set_file)\n",
    "with open(\"val_set\", \"rb\") as val_set_file:\n",
    "    user_val_Xs, movie_val_Xs, val_ys = pickle.load(val_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(*args, batch_size=1000, shuffle=True):\n",
    "    if batch_size == -1:\n",
    "        return [args]\n",
    "    \n",
    "    num_elems = len(args[0])\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.arange(num_elems, dtype=np.int64)\n",
    "        np.random.shuffle(shuffle_indices)\n",
    "        for i in range(0, num_elems, batch_size):\n",
    "            array_indices = shuffle_indices[i: i+batch_size]\n",
    "            try:\n",
    "                yield [arg[array_indices] for arg in args]\n",
    "            except:\n",
    "                raise Exception(\"args to batchify must be numpy arrays if shuffle True\")\n",
    "    else:\n",
    "        for i in range(0, num_elems, batch_size):\n",
    "            yield [arg[i: i+batch_size] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all movies in test data accounted for in genre information dataset\n",
    "\n",
    "# no_genre_count = 0\n",
    "# total = 0\n",
    "\n",
    "# with open(test_set, newline=\"\") as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "#     for rating in tqdm(reader):\n",
    "#         if movieid_mid_lookup[int(float(rating[\"movieId\"]))] not in movie_genres_one_hot:\n",
    "#             no_genre_count += 1\n",
    "#         total += 1\n",
    "\n",
    "# print(f\"{no_genre_count}/{total} entries in the test data doesn't have genre info ({no_genre_count/total}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # no memory - implicitly calculating user movie matrix from now on\n",
    "\n",
    "# movie_embeddings = tf.Variable(tf.random_normal([5, NUM_MOVIES], stddev=0.03, dtype=tf.float32))\n",
    "# user_embeddings = tf.Variable(tf.random_normal([NUM_USERS, 5], stddev=0.03, dtype=tf.float32))\n",
    "# movie_bias = tf.Variable(tf.random_normal([1, NUM_MOVIES], stddev=0.03, dtype=tf.float32))\n",
    "# user_bias = tf.Variable(tf.random_normal([NUM_USERS, 1], stddev=0.03, dtype=tf.float32))\n",
    "\n",
    "# user_movie_score = tf.tensordot(user_embeddings, movie_embeddings, axes = 1)+.14*tf.tile(movie_bias, [NUM_USERS, 1]) +.87*tf.tile(user_bias, [1, NUM_MOVIES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "movie_embedding_rows shape (?, 40)\n",
      "(?, 40) (?, 40) (?, 1)\n",
      "input layer shape (?, 142)\n",
      "pred_y shape (?, 1)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 40\n",
    "assert embedding_dim > 20\n",
    "\n",
    "movie_genre_embeddings = tf.placeholder(dtype=tf.float32, shape=[None, 20])\n",
    "movie_embeddings = tf.Variable(tf.contrib.layers.xavier_initializer()([NUM_MOVIES, embedding_dim]))\n",
    "user_embeddings = tf.Variable(tf.contrib.layers.xavier_initializer()([NUM_USERS, embedding_dim]))\n",
    "movie_bias = tf.Variable(tf.random_normal([NUM_MOVIES], stddev=0.03, dtype=tf.float32))\n",
    "user_bias = tf.Variable(tf.random_normal([NUM_USERS], stddev=0.03, dtype=tf.float32))\n",
    "\n",
    "user_slice_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1]) # columns vectors to do tensor slicing\n",
    "movie_slice_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1]) # columns vectors to do tensor slicing\n",
    "user_bias_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1]) # columns vectors to do tensor slicing\n",
    "movie_bias_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1]) # columns vectors to do tensor slicing\n",
    "\n",
    "user_embedding_columns = tf.reshape(tf.gather_nd(user_embeddings, user_slice_idxs), [-1, embedding_dim])\n",
    "movie_embedding_rows = tf.reshape(tf.gather_nd(movie_embeddings, movie_slice_idxs), [-1, embedding_dim])\n",
    "print(\"movie_embedding_rows shape\", movie_embedding_rows.shape)\n",
    "\n",
    "user_slice_bias = tf.reshape(tf.gather_nd(user_bias, user_slice_idxs), [-1, 1])\n",
    "movie_slice_bias = tf.reshape(tf.gather_nd(movie_bias, movie_slice_idxs), [-1, 1])\n",
    "# print(\"user_slice_bias shape\", user_slice_bias.shape)\n",
    "\n",
    "# print((user_embedding_columns * tf.concat((movie_embedding_rows, movie_genre_embeddings), axis=1)).shape)\n",
    "# print(movie_slice_bias.shape)\n",
    "# print(user_slice_bias.shape)\n",
    "\n",
    "input_layer = tf.concat((\n",
    "    movie_embedding_rows * user_embedding_columns,\n",
    "    movie_embedding_rows,\n",
    "    movie_genre_embeddings,\n",
    "    user_embedding_columns,\n",
    "    user_slice_bias,\n",
    "    movie_slice_bias), axis=1)\n",
    "print(movie_embedding_rows.shape, user_embedding_columns.shape, user_slice_bias.shape)\n",
    "print(\"input layer shape\", input_layer.shape)\n",
    "\n",
    "W1 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[embedding_dim*3+2+20, 40], dtype=tf.float32))\n",
    "b1 = tf.Variable(initial_value=np.zeros(shape=[40], dtype=np.float32))\n",
    "l1 = tf.nn.relu(tf.matmul(input_layer, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[40, 20], dtype=tf.float32))\n",
    "b2 = tf.Variable(initial_value=np.zeros(shape=[20], dtype=np.float32))\n",
    "l2 = tf.nn.relu(tf.matmul(l1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[20, 1], dtype=tf.float32))\n",
    "b3 = tf.Variable(initial_value=np.zeros(shape=[1], dtype=np.float32))\n",
    "pred_y =tf.nn.sigmoid(tf.matmul(l2, W3) + b3)*2-1\n",
    "\n",
    "print(\"pred_y shape\", pred_y.shape)\n",
    "\n",
    "# embedding_pred_vectors = tf.reshape(tf.reduce_sum(user_embedding_columns * tf.concat((movie_embedding_rows, movie_genre_embeddings), axis=1), axis=1), (-1, 1))\n",
    "# pred_y = embedding_pred_vectors + .14*movie_slice_bias + .87*user_slice_bias\n",
    "# print(embedding_pred_vectors.shape)\n",
    "# print(pred_y.shape)\n",
    "\n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "all_weights = [W1, W2, W3, b1, b2, b3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing train val genres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb9d7ebe1234e29bbf589e8dee57330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11946576), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7b8e427b84460a8f6831635353d00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3999236), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "train loss 1.0164378 pred_ys [[ 0.01772642]\n",
      " [ 0.07039428]\n",
      " [ 0.02488565]\n",
      " [ 0.13239658]\n",
      " [-0.00747168]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.009111186 mse_loss_term 1.0164378\n",
      "val loss 0.9926707\n",
      "val acc 0.5171700294756298\n",
      "val auc 0.591542827943154\n",
      "training\n",
      "train loss 0.98980594 pred_ys [[-0.00808752]\n",
      " [ 0.06464744]\n",
      " [-0.03564918]\n",
      " [-0.57050467]\n",
      " [-0.41296172]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.009838465 mse_loss_term 0.98980594\n",
      "val loss 0.95427936\n",
      "val acc 0.6070521969696212\n",
      "val auc 0.7177634725715102\n",
      "training\n",
      "train loss 0.91733825 pred_ys [[ 0.0916903 ]\n",
      " [ 0.6602273 ]\n",
      " [ 0.28029442]\n",
      " [-0.1444459 ]\n",
      " [-0.0903998 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.010942727 mse_loss_term 0.91733825\n",
      "val loss 0.8159217\n",
      "val acc 0.5876207355604921\n",
      "val auc 0.7494237349099935\n",
      "training\n",
      "train loss 0.7768383 pred_ys [[-0.2832886 ]\n",
      " [ 0.60788155]\n",
      " [ 0.05063021]\n",
      " [-0.4607445 ]\n",
      " [-0.4050036 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.012757523 mse_loss_term 0.7768383\n",
      "val loss 0.8333317\n",
      "val acc 0.6066268657313547\n",
      "val auc 0.7632269709835333\n",
      "training\n",
      "train loss 0.7738984 pred_ys [[-0.5949847 ]\n",
      " [ 0.60386944]\n",
      " [-0.24172366]\n",
      " [-0.70061857]\n",
      " [-0.67464644]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.015399885 mse_loss_term 0.7738984\n",
      "val loss 0.8164565\n",
      "val acc 0.6447456464184659\n",
      "val auc 0.7708303807666953\n",
      "training\n",
      "train loss 0.75045955 pred_ys [[-0.47724628]\n",
      " [ 0.7282541 ]\n",
      " [-0.13697022]\n",
      " [-0.7420709 ]\n",
      " [-0.75360674]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.017952487 mse_loss_term 0.75045955\n",
      "val loss 0.78783405\n",
      "val acc 0.6595599759554075\n",
      "val auc 0.7759552831164616\n",
      "training\n",
      "train loss 0.72591436 pred_ys [[-0.27358472]\n",
      " [ 0.7489748 ]\n",
      " [ 0.00902307]\n",
      " [-0.72733927]\n",
      " [-0.75306755]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.020204484 mse_loss_term 0.72591436\n",
      "val loss 0.7710085\n",
      "val acc 0.650050409628239\n",
      "val auc 0.7796643702552557\n",
      "training\n",
      "train loss 0.7162951 pred_ys [[-0.15937531]\n",
      " [ 0.66829515]\n",
      " [ 0.05488479]\n",
      " [-0.7049988 ]\n",
      " [-0.7383421 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.022189992 mse_loss_term 0.7162951\n",
      "val loss 0.76604253\n",
      "val acc 0.6205382728101068\n",
      "val auc 0.7808847870236973\n",
      "training\n",
      "train loss 0.71695906 pred_ys [[-0.12459034]\n",
      " [ 0.5568429 ]\n",
      " [ 0.05713356]\n",
      " [-0.7182469 ]\n",
      " [-0.7543445 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.024057 mse_loss_term 0.71695906\n",
      "val loss 0.76449436\n",
      "val acc 0.6082821818967423\n",
      "val auc 0.7807207413001347\n",
      "training\n",
      "train loss 0.71547765 pred_ys [[-0.12864405]\n",
      " [ 0.5839571 ]\n",
      " [ 0.06056929]\n",
      " [-0.7863666 ]\n",
      " [-0.8166021 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.025953395 mse_loss_term 0.71547765\n",
      "val loss 0.76422715\n",
      "val acc 0.6242782371432943\n",
      "val auc 0.780501750159612\n",
      "training\n",
      "train loss 0.7100896 pred_ys [[-0.1641233 ]\n",
      " [ 0.713462  ]\n",
      " [ 0.06486893]\n",
      " [-0.8690773 ]\n",
      " [-0.8879984 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.027935348 mse_loss_term 0.7100896\n",
      "val loss 0.767554\n",
      "val acc 0.6369841639753193\n",
      "val auc 0.7813345284603774\n",
      "training\n",
      "train loss 0.70796037 pred_ys [[-0.22320533]\n",
      " [ 0.7843008 ]\n",
      " [ 0.03742087]\n",
      " [-0.92040455]\n",
      " [-0.93192786]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.02992265 mse_loss_term 0.70796037\n",
      "val loss 0.76840353\n",
      "val acc 0.6423404370234714\n",
      "val auc 0.7830207127873134\n",
      "training\n",
      "train loss 0.70466894 pred_ys [[-0.29333174]\n",
      " [ 0.8049536 ]\n",
      " [-0.0101983 ]\n",
      " [-0.940435  ]\n",
      " [-0.9489216 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.031872544 mse_loss_term 0.70466894\n",
      "val loss 0.76507014\n",
      "val acc 0.641970366339971\n",
      "val auc 0.7847245158625085\n",
      "training\n",
      "train loss 0.6990546 pred_ys [[-0.36031514]\n",
      " [ 0.78717065]\n",
      " [-0.0426349 ]\n",
      " [-0.94327784]\n",
      " [-0.9509733 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.033828065 mse_loss_term 0.6990546\n",
      "val loss 0.75953573\n",
      "val acc 0.6384399420289275\n",
      "val auc 0.785977647182777\n",
      "training\n",
      "train loss 0.6931668 pred_ys [[-0.41434813]\n",
      " [ 0.7395514 ]\n",
      " [-0.07987642]\n",
      " [-0.93459266]\n",
      " [-0.94283944]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.035842277 mse_loss_term 0.6931668\n",
      "val loss 0.7544665\n",
      "val acc 0.6349212699625628\n",
      "val auc 0.7867656368520628\n",
      "training\n",
      "train loss 0.68895924 pred_ys [[-0.4501701 ]\n",
      " [ 0.68788743]\n",
      " [-0.12711227]\n",
      " [-0.91615796]\n",
      " [-0.92574054]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.03798022 mse_loss_term 0.68895924\n",
      "val loss 0.7513104\n",
      "val acc 0.6361645074209173\n",
      "val auc 0.7874261289780845\n",
      "training\n",
      "train loss 0.6863824 pred_ys [[-0.4736166 ]\n",
      " [ 0.66931677]\n",
      " [-0.17919028]\n",
      " [-0.89332235]\n",
      " [-0.90388316]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.04026318 mse_loss_term 0.6863824\n",
      "val loss 0.7498905\n",
      "val acc 0.6426137392241918\n",
      "val auc 0.7881972250788122\n",
      "training\n",
      "train loss 0.6841469 pred_ys [[-0.49883002]\n",
      " [ 0.6937909 ]\n",
      " [-0.22792512]\n",
      " [-0.8803378 ]\n",
      " [-0.890669  ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.04269993 mse_loss_term 0.6841469\n",
      "val loss 0.74926513\n",
      "val acc 0.6485843796165067\n",
      "val auc 0.7889030976146829\n",
      "training\n",
      "train loss 0.6813971 pred_ys [[-0.5338709 ]\n",
      " [ 0.7306912 ]\n",
      " [-0.28030992]\n",
      " [-0.88995105]\n",
      " [-0.8988073 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.045293413 mse_loss_term 0.6813971\n",
      "val loss 0.7492391\n",
      "val acc 0.6491957463875601\n",
      "val auc 0.7892623886596659\n",
      "training\n",
      "train loss 0.67839265 pred_ys [[-0.5788065 ]\n",
      " [ 0.7494652 ]\n",
      " [-0.33131617]\n",
      " [-0.91478854]\n",
      " [-0.92154354]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.048018474 mse_loss_term 0.67839265\n",
      "val loss 0.7506372\n",
      "val acc 0.6461784200782349\n",
      "val auc 0.7892542801574239\n",
      "training\n",
      "train loss 0.6763769 pred_ys [[-0.6204773 ]\n",
      " [ 0.7470838 ]\n",
      " [-0.362854  ]\n",
      " [-0.93700504]\n",
      " [-0.94201165]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.050867077 mse_loss_term 0.6763769\n",
      "val loss 0.7526752\n",
      "val acc 0.647498672246399\n",
      "val auc 0.7893758050522366\n",
      "training\n",
      "train loss 0.6745239 pred_ys [[-0.6459666 ]\n",
      " [ 0.75264966]\n",
      " [-0.35669947]\n",
      " [-0.94782925]\n",
      " [-0.9518932 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.05371679 mse_loss_term 0.6745239\n",
      "val loss 0.75369495\n",
      "val acc 0.6536778524698217\n",
      "val auc 0.7897788157695687\n",
      "training\n",
      "train loss 0.67157876 pred_ys [[-0.6495092 ]\n",
      " [ 0.77458334]\n",
      " [-0.30839592]\n",
      " [-0.9469861 ]\n",
      " [-0.9508296 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.056544956 mse_loss_term 0.67157876\n",
      "val loss 0.7530474\n",
      "val acc 0.6576153545327158\n",
      "val auc 0.7900649132845493\n",
      "training\n",
      "train loss 0.6681051 pred_ys [[-0.63492554]\n",
      " [ 0.77996635]\n",
      " [-0.22783351]\n",
      " [-0.9349729 ]\n",
      " [-0.93909585]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.05940402 mse_loss_term 0.6681051\n",
      "val loss 0.7507626\n",
      "val acc 0.6563393608179162\n",
      "val auc 0.790156438800374\n",
      "training\n",
      "train loss 0.66458124 pred_ys [[-0.61174893]\n",
      " [ 0.7578089 ]\n",
      " [-0.1550101 ]\n",
      " [-0.91352516]\n",
      " [-0.91815543]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.062328897 mse_loss_term 0.66458124\n",
      "val loss 0.74826705\n",
      "val acc 0.6524613701216933\n",
      "val auc 0.7903962179159107\n",
      "training\n",
      "train loss 0.66154134 pred_ys [[-0.5921895 ]\n",
      " [ 0.7283546 ]\n",
      " [-0.10534477]\n",
      " [-0.8911181 ]\n",
      " [-0.8961137 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.06532683 mse_loss_term 0.66154134\n",
      "val loss 0.74671465\n",
      "val acc 0.6538961441635353\n",
      "val auc 0.791148891807375\n",
      "training\n",
      "train loss 0.6581943 pred_ys [[-0.5842519 ]\n",
      " [ 0.7294011 ]\n",
      " [-0.0715943 ]\n",
      " [-0.8812234 ]\n",
      " [-0.88621455]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.06839058 mse_loss_term 0.6581943\n",
      "val loss 0.7464437\n",
      "val acc 0.6593526863630953\n",
      "val auc 0.7920253297939901\n",
      "training\n",
      "train loss 0.6544897 pred_ys [[-0.5909894 ]\n",
      " [ 0.74967563]\n",
      " [-0.05287969]\n",
      " [-0.8881352 ]\n",
      " [-0.89272195]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.071603596 mse_loss_term 0.6544897\n",
      "val loss 0.74683887\n",
      "val acc 0.6611590313749927\n",
      "val auc 0.7925264934886003\n",
      "training\n",
      "train loss 0.6505419 pred_ys [[-0.6098946 ]\n",
      " [ 0.7397733 ]\n",
      " [-0.04130888]\n",
      " [-0.9021382 ]\n",
      " [-0.90611094]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.07507209 mse_loss_term 0.6505419\n",
      "val loss 0.747492\n",
      "val acc 0.6590611306759591\n",
      "val auc 0.7925577335692671\n",
      "training\n",
      "train loss 0.6460579 pred_ys [[-0.63142794]\n",
      " [ 0.69035435]\n",
      " [-0.03344649]\n",
      " [-0.91234386]\n",
      " [-0.91599894]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.07883945 mse_loss_term 0.6460579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.7490444\n",
      "val acc 0.6595932323073707\n",
      "val auc 0.7922422608983062\n",
      "training\n",
      "train loss 0.6410931 pred_ys [[-0.6478772 ]\n",
      " [ 0.64186287]\n",
      " [-0.02477098]\n",
      " [-0.9135187 ]\n",
      " [-0.91690075]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.08287724 mse_loss_term 0.6410931\n",
      "val loss 0.7514176\n",
      "val acc 0.6656756440480132\n",
      "val auc 0.7919489336070575\n",
      "training\n",
      "train loss 0.6355074 pred_ys [[-0.66371506]\n",
      " [ 0.63720036]\n",
      " [-0.01878726]\n",
      " [-0.9055966 ]\n",
      " [-0.90879935]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.08709913 mse_loss_term 0.6355074\n",
      "val loss 0.7533398\n",
      "val acc 0.669577889376871\n",
      "val auc 0.7916580547960306\n",
      "training\n",
      "train loss 0.6294983 pred_ys [[-0.68390214]\n",
      " [ 0.6436925 ]\n",
      " [-0.00541711]\n",
      " [-0.8946968 ]\n",
      " [-0.8975044 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.09145888 mse_loss_term 0.6294983\n",
      "val loss 0.75431204\n",
      "val acc 0.6685759480060691\n",
      "val auc 0.7911796756336915\n",
      "training\n",
      "train loss 0.6228709 pred_ys [[-0.7112639 ]\n",
      " [ 0.62995243]\n",
      " [ 0.01707554]\n",
      " [-0.89226395]\n",
      " [-0.8925861 ]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.09592535 mse_loss_term 0.6228709\n",
      "val loss 0.7554377\n",
      "val acc 0.6678360566868272\n",
      "val auc 0.7910544296347803\n",
      "training\n",
      "train loss 0.61563635 pred_ys [[-0.7359628 ]\n",
      " [ 0.61855304]\n",
      " [ 0.04514658]\n",
      " [-0.90402645]\n",
      " [-0.89949566]] true_ys [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n",
      "l2_loss_term 0.10045312 mse_loss_term 0.61563635\n",
      "val loss 0.7577328\n",
      "val acc 0.6697701761036358\n",
      "val auc 0.7912170080875461\n"
     ]
    }
   ],
   "source": [
    "# def compute_slices(user_Xs, movie_Xs, embedding_dim):\n",
    "#     user_slice_idxs = [[user_X] for user_X in tqdm(user_Xs)]\n",
    "#     movie_slice_idxs = [[movie_X] for movie_X in tqdm(movie_Xs)]\n",
    "    \n",
    "    \n",
    "#     return [np.array(user_slice_idxs).reshape([-1, 1]), np.array(movie_slice_idxs).reshape([-1, 1])]\n",
    "\n",
    "learning_rate = .05\n",
    "epochs = 35\n",
    "\n",
    "l2_loss_term = .0001 * sum([tf.reduce_sum(tf.reshape(weight*weight, [-1])) for weight in all_weights])\n",
    "mse_loss_term = tf.reduce_mean(tf.squared_difference(pred_y, y_true))\n",
    "loss = mse_loss_term\n",
    "# + l2_loss_term\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# print(\"computing train val slice indices\")\n",
    "# slice_indices = compute_slices(user_Xs, movie_Xs, embedding_dim)\n",
    "# user_slice, movie_slice = slice_indices\n",
    "\n",
    "# val_slice_indices = compute_slices(user_val_Xs, movie_val_Xs, embedding_dim)\n",
    "# user_val_slice, movie_val_slice = val_slice_indices\n",
    "\n",
    "print(\"computing train val genres\")\n",
    "train_genres = np.array([movie_genres_one_hot[x[0]] for x in tqdm(movie_Xs)])\n",
    "val_genres = np.array([movie_genres_one_hot[x[0]] for x in tqdm(movie_val_Xs)])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), leave=False):\n",
    "        feed_dict = {user_slice_idxs: user_Xs, \n",
    "                     movie_slice_idxs: movie_Xs, \n",
    "                     movie_genre_embeddings: train_genres,\n",
    "                     y_true: ys}\n",
    "        print(\"training\")\n",
    "        outs = (train_step, loss, pred_y[:5], l2_loss_term, mse_loss_term)\n",
    "        _, lossval, pred_y_val, l2_loss_term_val, mse_loss_term_val = sess.run(outs, feed_dict=feed_dict)\n",
    "        print(\"train loss\", lossval, \"pred_ys\", pred_y_val, \"true_ys\", ys[:5])\n",
    "        print(\"l2_loss_term\", l2_loss_term_val, \"mse_loss_term\", mse_loss_term_val)\n",
    "        \n",
    "        feed_dict = {user_slice_idxs: user_val_Xs, \n",
    "                         movie_slice_idxs: movie_val_Xs,\n",
    "                         movie_genre_embeddings: val_genres,\n",
    "                         y_true: val_ys}\n",
    "        val_y_pred, val_loss_val = sess.run((pred_y, loss), feed_dict=feed_dict)\n",
    "        print(\"val loss\", val_loss_val)\n",
    "        print(\"val acc\", metrics.accuracy_score(val_ys, [(1 if y > .5 else -1) for y in val_y_pred]))\n",
    "        fpr, tpr, _ = metrics.roc_curve(val_ys, val_y_pred)\n",
    "        print(\"val auc\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_val_correct = [((1 if pred > .5 else -1) == true) for pred, true in zip(val_y_pred, val_ys)]\n",
    "print(\"val acc\", sum(total_val_correct)/len(val_ys))\n",
    "print(\"val acc?\", metrics.accuracy_score([(1 if (i > .5) else -1) for i in val_y_pred], val_ys.T[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1] [ 1 -1 -1  1  1]\n"
     ]
    }
   ],
   "source": [
    "print([(1 if i else -1) for i in val_y_pred][:5], val_ys.T[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3999236, 1) (3999236, 1) (3999236,)\n",
      "(11946576, 1) (11946576, 1) (11946576,)\n"
     ]
    }
   ],
   "source": [
    "val_slice_indices = compute_slices(user_val_Xs, movie_val_Xs, embedding_dim)\n",
    "user_val_slice, movie_val_slice = val_slice_indices\n",
    "\n",
    "print(user_val_slice.shape, movie_val_slice.shape, np.array(val_ys).shape)\n",
    "\n",
    "slice_indices = compute_slices(user_Xs, movie_Xs, embedding_dim)\n",
    "user_slice, movie_slice = slice_indices\n",
    "\n",
    "print(user_slice.shape, movie_slice.shape, np.array(ys).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11946576 11946576\n",
      "3999236 3999236\n"
     ]
    }
   ],
   "source": [
    "print(len(user_Xs), len(ys))\n",
    "print(len(user_val_Xs), len(val_ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
